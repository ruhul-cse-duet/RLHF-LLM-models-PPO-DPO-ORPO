# RLHF Modular Project - PPO, DPO & ORPO

A comprehensive, production-ready implementation of Reinforcement Learning from Human Feedback (RLHF) techniques for Large Language Models.

## ğŸš€ Supported Techniques

### 1. **PPO (Proximal Policy Optimization)**
- Traditional RLHF approach
- Requires separate reward model
- Stable policy updates with clipping
- Best for: Complex reward functions

### 2. **DPO (Direct Preference Optimization)**
- Simplified RLHF without reward model
- Direct optimization on preference pairs
- More stable than PPO
- Best for: Preference-based alignment

### 3. **ORPO (Odds Ratio Preference Optimization)**
- Single-stage training (SFT + Preference learning)
- Most efficient approach
- Strong performance with less compute
- Best for: Resource-constrained environments

## ğŸ“ Project Structure

```
rlhf_modular/
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_config.py    # Model configurations
â”‚   â”œâ”€â”€ training_config.py # Training hyperparameters
â”‚   â””â”€â”€ data_config.py     # Dataset configurations
â”œâ”€â”€ data/                   # Data processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py     # Dataset loading
â”‚   â”œâ”€â”€ preprocessor.py    # Data preprocessing
â”‚   â””â”€â”€ collators.py       # Data collators
â”œâ”€â”€ models/                 # Model components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_loader.py    # Model initialization
â”‚   â”œâ”€â”€ peft_config.py     # LoRA/QLoRA configs
â”‚   â””â”€â”€ reward_model.py    # Reward model (for PPO)
â”œâ”€â”€ trainers/               # Training logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_trainer.py    # Base trainer class
â”‚   â”œâ”€â”€ ppo_trainer.py     # PPO implementation
â”‚   â”œâ”€â”€ dpo_trainer.py     # DPO implementation
â”‚   â””â”€â”€ orpo_trainer.py    # ORPO implementation
â”œâ”€â”€ utils/                  # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py         # Evaluation metrics
â”‚   â”œâ”€â”€ logger.py          # Logging utilities
â”‚   â””â”€â”€ helpers.py         # Helper functions
â”œâ”€â”€ scripts/                # Execution scripts
â”‚   â”œâ”€â”€ train_ppo.py       # Train with PPO
â”‚   â”œâ”€â”€ train_dpo.py       # Train with DPO
â”‚   â”œâ”€â”€ train_orpo.py      # Train with ORPO
â”‚   â””â”€â”€ evaluate.py        # Model evaluation
â”œâ”€â”€ notebooks/              # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_ppo_training.ipynb
â”‚   â”œâ”€â”€ 03_dpo_training.ipynb
â”‚   â””â”€â”€ 04_orpo_training.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/ruhul-cse-duet/RLHF-LLM-models-PPO-DPO-ORPO.git
cd RLHF-LLM-models-PPO-DPO-ORPO

# Install dependencies
pip install -r requirements.txt
```

## ğŸ’» Quick Start

### Train with DPO
```bash
python scripts/train_dpo.py \
    --model_name mistralai/Mistral-7B-v0.1 \
    --dataset HuggingFaceH4/ultrafeedback_binarized \
    --output_dir ./outputs/dpo \
    --num_epochs 3
```

### Train with ORPO
```bash
python scripts/train_orpo.py \
    --model_name mistralai/Mistral-7B-v0.1 \
    --dataset HuggingFaceH4/ultrafeedback_binarized \
    --output_dir ./outputs/orpo \
    --num_epochs 3
```

### Train with PPO
```bash
python scripts/train_ppo.py \
    --model_name gpt2 \
    --reward_model <reward-model-path> \
    --dataset <dataset-name> \
    --output_dir ./outputs/ppo
```

## ğŸ“Š Features

- âœ… Modular and extensible architecture
- âœ… Support for multiple RL techniques (PPO, DPO, ORPO)
- âœ… 4-bit quantization (QLoRA) support
- âœ… LoRA/QLoRA fine-tuning
- âœ… Mixed precision training
- âœ… Comprehensive logging and metrics
- âœ… Easy configuration management
- âœ… Production-ready code

## ğŸ“ˆ Performance Comparison

| Technique | Training Speed | Memory Usage | Performance | Complexity |
|-----------|---------------|--------------|-------------|------------|
| PPO       | â­â­          | â­â­          | â­â­â­â­     | â­â­â­â­â­    |
| DPO       | â­â­â­â­      | â­â­â­        | â­â­â­â­     | â­â­â­      |
| ORPO      | â­â­â­â­â­    | â­â­â­â­      | â­â­â­â­â­   | â­â­        |

## ğŸ“š Documentation

For detailed documentation, see:
- [Configuration Guide](docs/configuration.md)
- [Training Guide](docs/training.md)
- [API Reference](docs/api.md)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- Hugging Face TRL library
- Unsloth for fast training
- OpenAI for pioneering RLHF
- Anthropic for DPO research

## Author
[Md Ruhul Amin](https://www.linkedin.com/in/ruhul-duet-cse/);  
Email: ruhul.cse.duet@gmail.com